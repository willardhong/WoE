{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import math\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### weight of evidence ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://stackoverflow.com/questions/50352210/woe-and-iv-table-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iv(df, feature, target, pr=0):\n",
    "    lst = []\n",
    "\n",
    "    for i in range(df[feature].nunique()):\n",
    "        val = list(df[feature].unique())[i]\n",
    "        lst.append([feature, val, df[df[feature] == val].count()[feature], df[(df[feature] == val) & (df[target] == 1)].count()[feature]])\n",
    " \n",
    "    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'Pop', 'Bad'])\n",
    "    data['Good'] = data['Pop'] - data['Bad']\n",
    "    \n",
    "    data['Badx'] = data['Bad'] + 0.01\n",
    "    data['Goodx'] = data['Good'] + 0.01\n",
    "    \n",
    "    data['Pop'] = data['Good'] + data['Bad']\n",
    "    data['Popx'] = data['Goodx'] + data['Badx']\n",
    "    \n",
    "    data['Dist Good'] = round((data['Pop'] - data['Bad']) / (data['Pop'].sum() - data['Bad'].sum()), 4)\n",
    "    data['Dist Bad'] = round(data['Bad'] / data['Bad'].sum(), 4)\n",
    "    \n",
    "    data['Dist Goodx'] = (data['Popx'] - data['Badx']) / (data['Popx'].sum() - data['Badx'].sum())\n",
    "    data['Dist Badx'] = data['Badx'] / data['Badx'].sum()\n",
    "    \n",
    "    data['Dist Pop'] = round(data['Pop'] / data['Pop'].sum(), 4)\n",
    "    data['Bad Rate'] = round(data['Bad'] / data['Pop'], 4)\n",
    "    data['grp_score'] = round((data['Dist Good']/(data['Dist Good'] + data['Dist Bad']))*10, 2)\n",
    "    \n",
    "    data['WoE'] = np.log(data['Dist Goodx'] / data['Dist Badx'])\n",
    "    data['IV'] = data['WoE'] * (data['Dist Goodx'] - data['Dist Badx'])\n",
    "    \n",
    "    data['Efficiency'] =  abs(data['Dist Good'] - data['Dist Bad'])/2  \n",
    "    \n",
    "    data = data.sort_values(by=['Variable', 'Value'], ascending=True)\n",
    "    data = data.reset_index()\n",
    "    mydf = data[[\n",
    "       'Value',\n",
    "       'Good', 'Bad',\n",
    "       'Dist Good','Dist Bad',\n",
    "       'WoE',\n",
    "       'IV'\n",
    "    ]]\n",
    "\n",
    "    #mydf=pd.DataFrame(data=d)\n",
    "   \n",
    "    if pr == 1:\n",
    "        print(mydf)\n",
    "        \n",
    "    total_iv = data['IV'].sum()\n",
    "    print(total_iv)\n",
    "\n",
    "    #return data['IV'].values[0]\n",
    "    #return mydf.values\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "iris.columns = ['sepal_l', 'sepal_w', 'petal_l', 'petal_w', 'type']\n",
    "\n",
    "df = pd.get_dummies(iris.type)['Iris-setosa'].to_frame()\n",
    "iris = iris.merge(df, left_index=True, right_index=True)\n",
    "\n",
    "iris = iris.drop(['type','sepal_w','petal_l','petal_w'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = iris.rename(columns = {'Iris-setosa': 'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_l  target\n",
      "0      5.1       1\n",
      "1      4.9       1\n",
      "2      4.7       1\n",
      "3      4.6       1\n",
      "4      5.0       1\n"
     ]
    }
   ],
   "source": [
    "print(iris.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.166238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.524059</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.015551</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.746162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.821745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x  target\n",
       "0  2.166238       0\n",
       "1  0.524059       0\n",
       "2  2.015551       0\n",
       "3  0.746162       0\n",
       "4  0.821745       0"
      ]
     },
     "execution_count": 1072,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.DataFrame(np.random.normal(1, 1, 1000))\n",
    "t['target'] = 0\n",
    "u = pd.DataFrame(np.random.normal(3, 1, 1000))\n",
    "u['target'] = 1\n",
    "data = pd.concat([t, u])\n",
    "data.columns = ['x', 'target']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Value  Good  Bad  Dist Good  Dist Bad       WoE        IV\n",
      "0    [-inf, 0.334)   272    8      0.272     0.008  3.525148  0.930434\n",
      "1   [0.334, 0.546)    74    6      0.074     0.006  2.510775  0.170695\n",
      "2   [0.546, 0.594)    19    1      0.019     0.001  2.935015  0.052819\n",
      "3   [0.594, 0.683)    37    3      0.037     0.003  2.509248  0.085296\n",
      "4    [0.683, 0.77)    37    3      0.037     0.003  2.509248  0.085296\n",
      "5    [0.77, 0.895)    53    7      0.053     0.007  2.023143  0.093044\n",
      "6   [0.895, 1.256)   163   37      0.163     0.037  1.482623  0.186769\n",
      "7   [1.256, 1.291)    17    3      0.017     0.003  1.731861  0.024241\n",
      "8   [1.291, 1.724)   153   87      0.153     0.087  0.564480  0.037248\n",
      "9   [1.724, 1.889)    43   37      0.043     0.037  0.150245  0.000901\n",
      "10  [1.889, 2.127)    44   76      0.044     0.076 -0.546448  0.017482\n",
      "11   [2.127, 2.27)    21   59      0.021     0.059 -1.032708  0.039234\n",
      "12   [2.27, 2.381)    11   49      0.011     0.049 -1.493220  0.056730\n",
      "13  [2.381, 2.526)    15   65      0.015     0.065 -1.465824  0.073275\n",
      "14  [2.526, 2.688)    12   88      0.012     0.088 -1.991711  0.151337\n",
      "15  [2.688, 2.718)     3   17      0.003     0.017 -1.731861  0.024241\n",
      "16  [2.718, 2.851)    10   70      0.010     0.070 -1.945053  0.116678\n",
      "17  [2.851, 2.924)     4   36      0.004     0.036 -2.195005  0.070225\n",
      "18  [2.924, 3.043)     4   56      0.004     0.056 -2.636739  0.137080\n",
      "19  [3.043, 3.311)     5  115      0.005     0.115 -3.133583  0.344618\n",
      "20  [3.311, 3.451)     1   59      0.001     0.059 -4.067757  0.235878\n",
      "21    [3.451, inf)     2  118      0.002     0.118 -4.072635  0.472322\n",
      "3.4058424097480846\n"
     ]
    }
   ],
   "source": [
    "WOE_Binning(data, 'target', 'x', p_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>target</th>\n",
       "      <th>cat</th>\n",
       "      <th>tran</th>\n",
       "      <th>mybin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.711</td>\n",
       "      <td>0</td>\n",
       "      <td>(1.711, 1.737]</td>\n",
       "      <td>1.711</td>\n",
       "      <td>[1.626, 1.835)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.936</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.936, 0.965]</td>\n",
       "      <td>0.936</td>\n",
       "      <td>[0.84, 1.087)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.870</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.87, 0.903]</td>\n",
       "      <td>0.870</td>\n",
       "      <td>[0.84, 1.087)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.087</td>\n",
       "      <td>0</td>\n",
       "      <td>(1.087, 1.125]</td>\n",
       "      <td>1.087</td>\n",
       "      <td>[1.087, 1.418)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.664</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.664, 0.694]</td>\n",
       "      <td>0.664</td>\n",
       "      <td>[0.314, 0.84)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x  target             cat   tran           mybin\n",
       "0  1.711       0  (1.711, 1.737]  1.711  [1.626, 1.835)\n",
       "1  0.936       0  (0.936, 0.965]  0.936   [0.84, 1.087)\n",
       "2  0.870       0   (0.87, 0.903]  0.870   [0.84, 1.087)\n",
       "3  1.087       0  (1.087, 1.125]  1.087  [1.087, 1.418)\n",
       "4  0.664       0  (0.664, 0.694]  0.664   [0.314, 0.84)"
      ]
     },
     "execution_count": 1058,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based loosely on https://github.com/jstephenj14/Monotonic-WOE-Binning-Algorithm/blob/master/Monotonic%20WOE%20Binning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WOE_Binning(data, Y, attr, sign = True, p_threshold=0.05):\n",
    "    #defaults_threshold = 1\n",
    "    #column = dataset.columns[dataset.columns != Y][0]\n",
    "    \n",
    "    #df = data.copy()\n",
    "    data['cat'] = pd.qcut(data[attr], 100)\n",
    "    data[attr] = data['cat'].apply(lambda x : x.left) \n",
    "    \n",
    "    column = data[attr]\n",
    "    \n",
    "    summary = data.groupby([column]).agg({Y:{\"means\":\"mean\",\"nsamples\":\"size\",\"std_dev\":\"std\"}})\n",
    "    summary.columns = summary.columns.droplevel(level=0)\n",
    "    summary = summary[[\"means\",\"nsamples\",\"std_dev\"]]\n",
    "    summary = summary.reset_index()\n",
    "    summary[\"del_flag\"] = 0\n",
    "    summary[\"std_dev\"] = summary[\"std_dev\"].fillna(0)\n",
    "    summary = summary.sort_values([attr],ascending = sign)\n",
    "    summary.reset_index(inplace = True, drop = True) \n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        if i > len(summary)-2:\n",
    "            break\n",
    "        else:\n",
    "            i = i + 1;\n",
    "            x = i - 1;\n",
    "           \n",
    "            n_x = summary.iloc[x].nsamples\n",
    "            n_i = summary.iloc[i].nsamples\n",
    "            \n",
    "            n = n_x + n_i\n",
    "            mean_x = summary.iloc[x].means\n",
    "            mean_i = summary.iloc[i].means\n",
    "            \n",
    "            var_x = (summary.iloc[x].std_dev)**2\n",
    "            var_i = (summary.iloc[i].std_dev)**2\n",
    "            \n",
    "            #std deviation\n",
    "            s = np.sqrt((var_x/n_x + var_i/n_i)/2)\n",
    "            p = 0\n",
    "            if s != 0:\n",
    "                t = (mean_x - mean_i)/s\n",
    "                df = 2*(n_x + n_i) - 2\n",
    "                p = 1 - stats.t.cdf(t,df=df)\n",
    "            if (p >= p_threshold) or (s == 0):\n",
    "                #absorb data\n",
    "                summary.loc[i, attr] = summary.loc[x, attr]\n",
    "                summary.loc[i, 'nsamples'] = n_x + n_i\n",
    "                summary.loc[i, 'means'] = (n_x*mean_x + n_i*mean_i)/n\n",
    "                summary.loc[i, 'std_dev'] = np.sqrt(((n_x*var_x)+(n_i*var_i))/n)\n",
    "                summary.loc[x, 'del_flag'] = 1        \n",
    "            continue\n",
    "            \n",
    "    summary = summary[summary['del_flag'] == 0]\n",
    "    summary = summary.reset_index()\n",
    "    l = [-math.inf]\n",
    "    for index, row in summary.iterrows():\n",
    "        if index > 0:\n",
    "            l.append(row[attr])\n",
    "    l.append(math.inf)    \n",
    "    #print(l)\n",
    "    data['mybin'] = pd.cut(x=data[attr], bins=l, right=False)\n",
    "    calc_iv(data, 'mybin', 'target', pr=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bin edges must be unique: array([4.3  , 4.4  , 4.4  , 4.547, 4.6  , 4.6  , 4.694, 4.743, 4.8  ,\n       4.8  , 4.8  , 4.9  , 4.9  , 4.9  , 4.9  , 5.   , 5.   , 5.   ,\n       5.   , 5.   , 5.   , 5.029, 5.1  , 5.1  , 5.1  , 5.1  , 5.1  ,\n       5.123, 5.2  , 5.2  , 5.27 , 5.4  , 5.4  , 5.4  , 5.4  , 5.5  ,\n       5.5  , 5.5  , 5.5  , 5.511, 5.6  , 5.6  , 5.6  , 5.607, 5.7  ,\n       5.7  , 5.7  , 5.7  , 5.7  , 5.8  , 5.8  , 5.8  , 5.8  , 5.8  ,\n       5.9  , 5.9  , 6.   , 6.   , 6.   , 6.   , 6.1  , 6.1  , 6.1  ,\n       6.1  , 6.2  , 6.2  , 6.234, 6.3  , 6.3  , 6.3  , 6.3  , 6.3  ,\n       6.328, 6.4  , 6.4  , 6.4  , 6.4  , 6.473, 6.5  , 6.5  , 6.52 ,\n       6.6  , 6.7  , 6.7  , 6.7  , 6.7  , 6.7  , 6.763, 6.8  , 6.861,\n       6.9  , 6.9  , 7.008, 7.157, 7.2  , 7.255, 7.408, 7.653, 7.7  ,\n       7.7  , 7.9  ]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1080-a80a67a38d3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mWOE_Binning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sepal_l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1069-1c82e40f1b7c>\u001b[0m in \u001b[0;36mWOE_Binning\u001b[0;34m(data, Y, attr, sign, p_threshold)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#df = data.copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36mqcut\u001b[0;34m(x, q, labels, retbins, precision, duplicates)\u001b[0m\n\u001b[1;32m    304\u001b[0m     fac, bins = _bins_to_cuts(x, bins, labels=labels,\n\u001b[1;32m    305\u001b[0m                               \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_lowest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                               dtype=dtype, duplicates=duplicates)\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     return _postprocess_for_cut(fac, bins, retbins, x_is_series,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36m_bins_to_cuts\u001b[0;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates)\u001b[0m\n\u001b[1;32m    330\u001b[0m             raise ValueError(\"Bin edges must be unique: {bins!r}.\\nYou \"\n\u001b[1;32m    331\u001b[0m                              \u001b[0;34m\"can drop duplicate edges by setting \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                              \"the 'duplicates' kwarg\".format(bins=bins))\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_bins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Bin edges must be unique: array([4.3  , 4.4  , 4.4  , 4.547, 4.6  , 4.6  , 4.694, 4.743, 4.8  ,\n       4.8  , 4.8  , 4.9  , 4.9  , 4.9  , 4.9  , 5.   , 5.   , 5.   ,\n       5.   , 5.   , 5.   , 5.029, 5.1  , 5.1  , 5.1  , 5.1  , 5.1  ,\n       5.123, 5.2  , 5.2  , 5.27 , 5.4  , 5.4  , 5.4  , 5.4  , 5.5  ,\n       5.5  , 5.5  , 5.5  , 5.511, 5.6  , 5.6  , 5.6  , 5.607, 5.7  ,\n       5.7  , 5.7  , 5.7  , 5.7  , 5.8  , 5.8  , 5.8  , 5.8  , 5.8  ,\n       5.9  , 5.9  , 6.   , 6.   , 6.   , 6.   , 6.1  , 6.1  , 6.1  ,\n       6.1  , 6.2  , 6.2  , 6.234, 6.3  , 6.3  , 6.3  , 6.3  , 6.3  ,\n       6.328, 6.4  , 6.4  , 6.4  , 6.4  , 6.473, 6.5  , 6.5  , 6.52 ,\n       6.6  , 6.7  , 6.7  , 6.7  , 6.7  , 6.7  , 6.763, 6.8  , 6.861,\n       6.9  , 6.9  , 7.008, 7.157, 7.2  , 7.255, 7.408, 7.653, 7.7  ,\n       7.7  , 7.9  ]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg"
     ]
    }
   ],
   "source": [
    "WOE_Binning(iris, 'target', 'sepal_l', p_threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "got this from github..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chimerge(data, attr, label, max_intervals):\n",
    "    distinct_vals = sorted(set(data[attr])) # Sort the distinct values\n",
    "    labels = sorted(set(data[label]))       # Get all possible labels\n",
    "    empty_count = {l: 0 for l in labels}    # A helper function for padding the Counter()\n",
    "    intervals = [[distinct_vals[i], distinct_vals[i]] for i in range(len(distinct_vals))] # Initialize the intervals for each attribute\n",
    "    while len(intervals) > max_intervals:   # While loop  \n",
    "        chi = []\n",
    "        for i in range(len(intervals)-1):\n",
    "            # Calculate the Chi2 value\n",
    "            obs0 = data[data[attr].between(intervals[i][0], intervals[i][1])]\n",
    "            obs1 = data[data[attr].between(intervals[i+1][0], intervals[i+1][1])]\n",
    "            total = len(obs0) + len(obs1)\n",
    "            count_0 = np.array([v for i, v in {**empty_count, **Counter(obs0[label])}.items()])\n",
    "            count_1 = np.array([v for i, v in {**empty_count, **Counter(obs1[label])}.items()])\n",
    "            count_total = count_0 + count_1\n",
    "            expected_0 = count_total*sum(count_0)/total\n",
    "            expected_1 = count_total*sum(count_1)/total\n",
    "            chi_ = (count_0 - expected_0)**2/expected_0 + (count_1 - expected_1)**2/expected_1\n",
    "            chi_ = np.nan_to_num(chi_) # Deal with the zero counts\n",
    "            chi.append(sum(chi_))      # Finally do the summation for Chi2\n",
    "        min_chi = min(chi)             # Find the minimal Chi2 for current iteration\n",
    "        for i, v in enumerate(chi): \n",
    "            if v == min_chi:\n",
    "                min_chi_index = i      # Find the index of the interval to be merged\n",
    "                break\n",
    "        new_intervals = []             # Prepare for the merged new data array\n",
    "        skip = False\n",
    "        done = False\n",
    "        for i in range(len(intervals)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i == min_chi_index and not done: # Merge the intervals\n",
    "                t = intervals[i] + intervals[i+1]\n",
    "                new_intervals.append([min(t), max(t)])\n",
    "                skip = True\n",
    "                done = True\n",
    "            else:\n",
    "                new_intervals.append(intervals[i])\n",
    "        intervals = new_intervals\n",
    "        \n",
    "   \n",
    "    l = [-math.inf]\n",
    "    for i in intervals:\n",
    "        l.append(i[1])\n",
    "    l.append(math.inf)\n",
    "    print(l)   \n",
    "    data['mybin'] = pd.cut(x=data[attr], bins=l, right=False)\n",
    "    calc_iv(data, 'mybin', 'target', pr=1)\n",
    "    #return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-inf, 0.0614, 0.242, 0.299, 1.002, 1.333, 1.553, 1.955, 2.341, 3.021, 3.823, inf]\n",
      "              Value  Good  Bad  Dist Good  Dist Bad       WoE        IV\n",
      "0    [-inf, 0.0614)   180    0      0.180     0.000  9.798183  1.763479\n",
      "1   [0.0614, 0.242)    57    3      0.057     0.003  2.941287  0.158812\n",
      "2    [0.242, 0.299)    19    1      0.019     0.001  2.935015  0.052824\n",
      "3    [0.299, 1.002)   273   27      0.273     0.027  2.313301  0.569010\n",
      "4    [1.002, 1.333)   143   37      0.143     0.037  1.351726  0.143267\n",
      "5    [1.333, 1.553)    87   33      0.087     0.033  0.969213  0.052332\n",
      "6    [1.553, 1.955)   109   91      0.109     0.091  0.180470  0.003248\n",
      "7    [1.955, 2.341)    65  135      0.065     0.135 -0.730808  0.051151\n",
      "8    [2.341, 3.021)    55  325      0.055     0.325 -1.776341  0.479559\n",
      "9    [3.021, 3.823)    12  328      0.012     0.328 -3.307304  1.044993\n",
      "10     [3.823, inf)     0   20      0.000     0.020 -7.601402  0.152011\n",
      "4.470686741599382\n"
     ]
    }
   ],
   "source": [
    "chimerge(data=data, attr='x', label='target', max_intervals=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the packages\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define 2 random distributions\n",
    "#Sample Size\n",
    "N = 10\n",
    "#Gaussian distributed data with mean = 2 and var = 1\n",
    "a = np.random.randn(N) + 2\n",
    "#Gaussian distributed data with with mean = 0 and var = 1\n",
    "b = np.random.randn(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the Standard Deviation\n",
    "#Calculate the variance to get the standard deviation\n",
    "\n",
    "#For unbiased max likelihood estimate we have to divide the var by N-1, and therefore the parameter ddof = 1\n",
    "var_a = a.var(ddof=1)\n",
    "var_b = b.var(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36602433831146336 0.7540508106223665\n"
     ]
    }
   ],
   "source": [
    "print(var_a, var_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 4.20921246738193\n",
      "p = 0.0005273169029311742\n",
      "t = 4.20921246738193\n",
      "p = 0.0005273169029312183\n"
     ]
    }
   ],
   "source": [
    "#std deviation\n",
    "s = np.sqrt((var_a + var_b)/2)\n",
    "#s\n",
    "\n",
    "## Calculate the t-statistics\n",
    "t = (a.mean() - b.mean())/(s*np.sqrt(2/N))\n",
    "\n",
    "## Compare with the critical t-value\n",
    "#Degrees of freedom\n",
    "df = 2*N - 2\n",
    "\n",
    "#p-value after comparison with the t \n",
    "p = 1 - stats.t.cdf(t,df=df)\n",
    "\n",
    "print(\"t = \" + str(t))\n",
    "print(\"p = \" + str(2*p))\n",
    "### You can see that after comparing the t statistic with the critical t value (computed internally) we get a good p value of 0.0005 and thus we reject the null hypothesis and thus it proves that the mean of the two distributions are different and statistically significant.\n",
    "\n",
    "## Cross Checking with the internal scipy function\n",
    "t2, p2 = stats.ttest_ind(a,b)\n",
    "print(\"t = \" + str(t2))\n",
    "print(\"p = \" + str(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
